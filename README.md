# "Truth-Seeking" LLM Assistant for the FDA Drug Approval Process

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://your-app-name.streamlit.app)

This project is a sophisticated, deployed AI assistant that answers questions about the FDA's drug approval process. It uses a Retrieval-Augmented Generation (RAG) pipeline to ensure every answer is based exclusively on a curated knowledge base of 12 official PDF documents, making it a reliable and trustworthy source of information.

## üöÄ Live Demo

You can try the live, interactive application here:
**[https://fda-llm-assistant.streamlit.app/](https://fda-llm-assistant.streamlit.app/)**

## ‚ú® Key Features

* **Live & Interactive:** Fully deployed as a public web application using Streamlit and Streamlit Community Cloud.
* **Factual Grounding:** The RAG architecture prevents the LLM from hallucinating by forcing it to answer questions based *only* on the provided documents.
* **Scalable Knowledge Base:** Ingests and processes multiple complex PDF documents, creating a single, searchable source of truth.
* **Efficient Retrieval:** Utilizes a FAISS vector database to perform rapid similarity searches, ensuring only the most relevant information is retrieved to answer a user's query.

## üõ†Ô∏è Tech Stack

* **Language:** Python
* **AI & ML:** LangChain, OpenAI (GPT-3.5-Turbo)
* **Vector Database:** FAISS (from Facebook AI)
* **Web Framework & Deployment:** Streamlit, Streamlit Community Cloud
* **Data Processing:** PyMuPDF

## ‚öôÔ∏è How the RAG Pipeline Works

The application operates on a RAG pipeline to ensure accuracy:

1.  **Indexing:** At startup, the app loads all 12 PDF documents, splits them into manageable chunks, converts each chunk into a numerical embedding, and stores them in a high-performance FAISS vector database.
2.  **Retrieval & Generation:** When a user asks a question, the app finds the most relevant text chunks from the database and passes them‚Äîalong with the question‚Äîto the LLM. The LLM then generates an answer based only on this provided context.

## üéì Project Evolution & Key Learnings

This project was a journey through the entire development lifecycle, from a local script to a fully deployed web application.

* **Initial Challenge:** A simple prototype immediately failed when scaling to the large knowledge base, hitting the LLM's `context_length_exceeded` limit. This was a critical lesson in the limitations of naive LLM prompting.
* **The Pivot:** This failure forced a pivot to a more sophisticated architecture. I designed and implemented a full RAG pipeline using LangChain and FAISS to handle the large dataset efficiently.
* **The Deployment Gauntlet:** Deploying the app to Streamlit Cloud presented a new set of real-world challenges that are invisible during local development. I successfully diagnosed and solved:
    * **API Secret Management:** Learning to use cloud-native secret management instead of local `.env` files.
    * **Dependency Conflicts:** Debugging a series of `ModuleNotFoundError` errors by meticulously creating a clean and correct `requirements.txt` file.
    * **Git Sync Issues:** Resolving `rejected push` and `bad object` errors by backing up local work and re-cloning from the remote source of truth.

This project taught me that building an AI model is only half the battle; deploying it, managing its environment, and debugging production-like issues are equally critical skills.

## üöÄ Local Installation & Usage

To run this project on your local machine, follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/mischaemerson/fda-llm-assistant.git](https://github.com/mischaemerson/fda-llm-assistant.git)
    cd fda-llm-assistant
    ```
2.  **Create and activate a virtual environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
3.  **Install the required dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Set up your environment variables:**
    Create a file named `.env` in the root of the project folder and add your OpenAI API key:
    ```
    OPENAI_API_KEY="your_secret_api_key_here"
    ```
5.  **Run the Streamlit application:**
    ```bash
    streamlit run streamlit_app.py
    ```

## üí° Future Improvements

* **Source Citing:** Enhance the model